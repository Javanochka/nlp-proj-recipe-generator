{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Training spaCy's NER Model to Identify Food Entities\n",
    "\n",
    "**Note:** This is a modified copy of [Isaac Aderogba's *Spacy Food Entities* notebook](https://deepnote.com/publish/2cc2d19c-c3ac-4321-8853-0bcf2ef565b3). It will be modified to fit our needs and create our own version.\n",
    "\n",
    "List of changes:\n",
    "  - Added the kernel specifications.\n",
    "  - Changed the revision dataset, as the original is unavailable. [[Scroll to section]](#Preparing-the-revision-data)\n",
    "  - Changed the created model's name and destination folder. [[Scroll to section]](#Saving-the-model)\n",
    "  - Add some tests based on a sample of our generated recipes.\n",
    "  - Check the newly saved model."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00000-340698f8-28eb-49e0-aaa6-b0c779ebc761",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As a [side project](https://niahealth.co/), I'm building an app that makes nutrition tracking as effortless as having a conversation.\n",
    "To do this, I'll be making use of [spaCy](https://spacy.io/) for natural language processing (NLP).\n",
    "\n",
    "In this notebook, we'll be training spaCy to identify `FOOD` entities from a body of text - a task known as named-entity recognition (NER). If all goes well, we should be able to identify the foods from the following sentences:\n",
    "\n",
    "- I decided to have `chocolate ice cream` as a little treat for myself.\n",
    "- I had a `hamburger` and `chips` for lunch today.\n",
    "- I ordered `basmati rice`, `leaf spinach` and `cheese` from Tesco yesterday.\n",
    "\n",
    "spaCy has a NER accuracy of [85.85%](https://spacy.io/usage/facts-figures), so something in that range would be nice for our `FOOD` entities. \n",
    "\n",
    "#### Approach\n",
    "We'll use the following approach:\n",
    "\n",
    "1. Generate sentences with `FOOD` entities.\n",
    "2. Generate sentences with **existing** [spaCy entities](https://spacy.io/usage/linguistic-features#named-entities) to avoid the [catastrophic forgetting](https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting) problem.\n",
    "3. Train spaCy NER with the existing entities and the custom `FOOD` entities. Stir until good enough.\n",
    "\n",
    "#### Results\n",
    "\n",
    "Category | Results\n",
    "---|---------\n",
    "`FOOD` Entities | 94.14%\n",
    "Existing Entities | 71.32%\n",
    "\n",
    "<br>\n",
    "\n",
    "See the Evaluation and Results section for a full breakdown."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00000-0c051677-d46b-47b9-99ce-330312600376",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00002-ea0a5597-ca7c-4244-af55-c876a462969f",
    "output_cleared": false,
    "source_hash": "3e7afbf0",
    "execution_millis": 4041,
    "execution_start": 1608746166524,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# download spacy language model\n",
    "# !python -m pip install spacy\n",
    "# !python -m spacy download en_core_web_lg"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001B[0m\nRequirement already satisfied: spacy in /opt/venv/lib/python3.7/site-packages (2.3.5)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (0.8.0)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (2.25.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (4.54.1)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/venv/lib/python3.7/site-packages (from spacy) (7.4.5)\nRequirement already satisfied: numpy>=1.15.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.19.4)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy) (2.0.5)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from spacy) (50.3.2)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (0.7.4)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.0.0)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy) (3.0.5)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.0.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.2)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.11.8)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.3.0)\nRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.0)\n\u001B[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\nRequirement already satisfied: en_core_web_lg==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.3.1/en_core_web_lg-2.3.1.tar.gz#egg=en_core_web_lg==2.3.1 in /opt/venv/lib/python3.7/site-packages (2.3.1)\nRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /opt/venv/lib/python3.7/site-packages (from en_core_web_lg==2.3.1) (2.3.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.25.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (4.54.1)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.8.0)\nRequirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (0.7.4)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.5)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.5)\nRequirement already satisfied: numpy>=1.15.0 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.19.4)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.1.3)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (50.3.2)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.0.0)\nRequirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (7.4.5)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.0.5)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (1.26.2)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (2020.11.8)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.3.0)\nRequirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_lg==2.3.1) (3.4.0)\n\u001B[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\n\u001B[38;5;2m✔ Download and installation successful\u001B[0m\nYou can now load the model via spacy.load('en_core_web_lg')\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00003-36147090-4896-430c-bfa0-1941a1c76126",
    "output_cleared": false,
    "source_hash": "bdad20c8",
    "execution_millis": 509,
    "execution_start": 1608746170569,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# import libraries\n",
    "import en_core_web_lg\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Food Data\n",
    "\n",
    "We'll be using food data from the USDA's [Branded Food's dataset](https://fdc.nal.usda.gov/download-datasets.html)."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00004-2299f998-a57b-496b-864d-01604de008aa",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the food data"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00005-6065fa77-431c-492d-b533-0aa100ff5265",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00000-e64a109f-7da3-43f5-bfc0-71a00dfb3e2e",
    "output_cleared": false,
    "source_hash": "c821cdef",
    "execution_millis": 447,
    "execution_start": 1608746171087,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# read in the food csv file\n",
    "food_df = pd.read_csv(\"food.csv\")\n",
    "\n",
    "# print row and column information\n",
    "food_df.head()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 3,
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "row_count": 5,
       "column_count": 5,
       "columns": [
        {
         "name": "fdc_id",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": 344604,
          "max": 344608,
          "histogram": [
           {
            "bin_start": 344604,
            "bin_end": 344604.4,
            "count": 1
           },
           {
            "bin_start": 344604.4,
            "bin_end": 344604.8,
            "count": 0
           },
           {
            "bin_start": 344604.8,
            "bin_end": 344605.2,
            "count": 1
           },
           {
            "bin_start": 344605.2,
            "bin_end": 344605.6,
            "count": 0
           },
           {
            "bin_start": 344605.6,
            "bin_end": 344606,
            "count": 0
           },
           {
            "bin_start": 344606,
            "bin_end": 344606.4,
            "count": 1
           },
           {
            "bin_start": 344606.4,
            "bin_end": 344606.8,
            "count": 0
           },
           {
            "bin_start": 344606.8,
            "bin_end": 344607.2,
            "count": 1
           },
           {
            "bin_start": 344607.2,
            "bin_end": 344607.6,
            "count": 0
           },
           {
            "bin_start": 344607.6,
            "bin_end": 344608,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "data_type",
         "dtype": "object",
         "stats": {
          "unique_count": 1,
          "nan_count": 0,
          "categories": [
           {
            "name": "branded_food",
            "count": 5
           }
          ]
         }
        },
        {
         "name": "description",
         "dtype": "object",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "categories": [
           {
            "name": "Tutturosso Green 14.5oz. NSA Italian Diced Tomatoes",
            "count": 1
           },
           {
            "name": "Tutturosso Green 14.5oz. Italian Diced Tomatoes",
            "count": 1
           },
           {
            "name": "3 others",
            "count": 3
           }
          ]
         }
        },
        {
         "name": "food_category_id",
         "dtype": "float64",
         "stats": {
          "unique_count": 0,
          "nan_count": 5,
          "min": null,
          "max": null,
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.1,
            "count": 0
           },
           {
            "bin_start": 0.1,
            "bin_end": 0.2,
            "count": 0
           },
           {
            "bin_start": 0.2,
            "bin_end": 0.30000000000000004,
            "count": 0
           },
           {
            "bin_start": 0.30000000000000004,
            "bin_end": 0.4,
            "count": 0
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.5,
            "count": 0
           },
           {
            "bin_start": 0.5,
            "bin_end": 0.6000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.6000000000000001,
            "bin_end": 0.7000000000000001,
            "count": 0
           },
           {
            "bin_start": 0.7000000000000001,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 0.9,
            "count": 0
           },
           {
            "bin_start": 0.9,
            "bin_end": 1,
            "count": 0
           }
          ]
         }
        },
        {
         "name": "publication_date",
         "dtype": "object",
         "stats": {
          "unique_count": 1,
          "nan_count": 0,
          "categories": [
           {
            "name": "2019-04-01",
            "count": 5
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows_top": [
        {
         "fdc_id": 344604,
         "data_type": "branded_food",
         "description": "Tutturosso Green 14.5oz. NSA Italian Diced Tomatoes",
         "food_category_id": "nan",
         "publication_date": "2019-04-01",
         "_deepnote_index_column": 0
        },
        {
         "fdc_id": 344605,
         "data_type": "branded_food",
         "description": "Tutturosso Green 14.5oz. Italian Diced Tomatoes",
         "food_category_id": "nan",
         "publication_date": "2019-04-01",
         "_deepnote_index_column": 1
        },
        {
         "fdc_id": 344606,
         "data_type": "branded_food",
         "description": "Honeysuckle White Fresh 97% Ground White Turkey",
         "food_category_id": "nan",
         "publication_date": "2019-04-01",
         "_deepnote_index_column": 2
        },
        {
         "fdc_id": 344607,
         "data_type": "branded_food",
         "description": "Honeysuckle White 97% Ground White Turkey",
         "food_category_id": "nan",
         "publication_date": "2019-04-01",
         "_deepnote_index_column": 3
        },
        {
         "fdc_id": 344608,
         "data_type": "branded_food",
         "description": "Honeysuckle Whtie 85% Ground Turkey",
         "food_category_id": "nan",
         "publication_date": "2019-04-01",
         "_deepnote_index_column": 4
        }
       ],
       "rows_bottom": null
      },
      "text/plain": "   fdc_id     data_type                                        description  \\\n0  344604  branded_food  Tutturosso Green 14.5oz. NSA Italian Diced Tom...   \n1  344605  branded_food    Tutturosso Green 14.5oz. Italian Diced Tomatoes   \n2  344606  branded_food    Honeysuckle White Fresh 97% Ground White Turkey   \n3  344607  branded_food          Honeysuckle White 97% Ground White Turkey   \n4  344608  branded_food                Honeysuckle Whtie 85% Ground Turkey   \n\n   food_category_id publication_date  \n0               NaN       2019-04-01  \n1               NaN       2019-04-01  \n2               NaN       2019-04-01  \n3               NaN       2019-04-01  \n4               NaN       2019-04-01  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fdc_id</th>\n      <th>data_type</th>\n      <th>description</th>\n      <th>food_category_id</th>\n      <th>publication_date</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>344604</td>\n      <td>branded_food</td>\n      <td>Tutturosso Green 14.5oz. NSA Italian Diced Tom...</td>\n      <td>NaN</td>\n      <td>2019-04-01</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>344605</td>\n      <td>branded_food</td>\n      <td>Tutturosso Green 14.5oz. Italian Diced Tomatoes</td>\n      <td>NaN</td>\n      <td>2019-04-01</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>344606</td>\n      <td>branded_food</td>\n      <td>Honeysuckle White Fresh 97% Ground White Turkey</td>\n      <td>NaN</td>\n      <td>2019-04-01</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>344607</td>\n      <td>branded_food</td>\n      <td>Honeysuckle White 97% Ground White Turkey</td>\n      <td>NaN</td>\n      <td>2019-04-01</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>344608</td>\n      <td>branded_food</td>\n      <td>Honeysuckle Whtie 85% Ground Turkey</td>\n      <td>NaN</td>\n      <td>2019-04-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see that the `description` column has the names for the foods we're interested in."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00006-1a5870a6-0fc1-4a5a-8851-91836143b7d0",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00007-773d6360-92ac-469a-8b67-c1049831fb1f",
    "output_cleared": false,
    "source_hash": "6d879995",
    "execution_millis": 3,
    "execution_start": 1608746171537,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# print the size \n",
    "food_df[\"description\"].size"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 4,
     "data": {
      "text/plain": "498182"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have way more rows than we need, so let's:\n",
    "- Disqualify foods with special characters (`+`,`&`, `!`, `,` and so on).\n",
    "- Filter out foods containing more than 3 words."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00007-9d788ab4-9189-4b34-992f-67fee142dde3",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00006-791566d9-ed51-4041-a0d7-75e424de39ec",
    "output_cleared": false,
    "source_hash": "6dab3e88",
    "execution_millis": 782,
    "execution_start": 1608746171564,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# diaqualify foods with special characters, lowercase and extract results from \"description\" column\n",
    "foods = food_df[food_df[\"description\"].str.contains(\"[^a-zA-Z.%\\- ]\") == False][\"description\"].apply(lambda food: food.lower())\n",
    "\n",
    "# filter out foods with more than 3 words, drop any duplicates\n",
    "foods = foods[foods.str.split().apply(len) <= 3].drop_duplicates()\n",
    "\n",
    "# print the remaining size\n",
    "foods.size"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 5,
     "data": {
      "text/plain": "39378"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we need to think about how we want our data to be distributed. By reducing to 3-worded food items, we effectively have food entities that look like this:\n",
    "\n",
    "- `hamburger` | 1-worded\n",
    "- `grilled cheese` | 2-worded\n",
    "- `chocolate ice cream` | 3-worded\n",
    "\n",
    "When feeding our training data into spaCy, we want to think about the bias we want spaCy to avoid. "
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00013-3ec39220-2472-465e-90f6-93fbfc0e8c50",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00014-c0cc3cfa-75dd-4588-aa26-43730a29b6f3",
    "output_cleared": false,
    "source_hash": "991434e8",
    "execution_millis": 213,
    "execution_start": 1608746172376,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# find one-worded, two-worded and three-worded foods\n",
    "one_worded_foods = foods[foods.str.split().apply(len) == 1]\n",
    "two_worded_foods = foods[foods.str.split().apply(len) == 2]\n",
    "three_worded_foods = foods[foods.str.split().apply(len) == 3]\n",
    "\n",
    "# create a bar plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar([1, 2, 3], [one_worded_foods.size, two_worded_foods.size, three_worded_foods.size])\n",
    "\n",
    "# label the x-axis instances\n",
    "ax.set_xticks([1, 2, 3])\n",
    "ax.set_xticklabels([\"one\", \"two\", \"three\"])\n",
    "\n",
    "# set the title and the xy-axis labels\n",
    "plt.title(\"Number of Words in Food Entities\")\n",
    "plt.xlabel(\"Number of Words\")\n",
    "plt.ylabel(\"Food Entities\")\n",
    "\n",
    "# display the plot\n",
    "plt.show()"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 720x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the majority of our food entities are multi-worded, spaCy would develop a bias for multi-worded foods. If we look back to our example \n",
    "of `grilled cheese`, it's not a big deal if spaCy identifies `cheese` instead of `grilled cheese`. It is a big deal if spaCy fails to identify `cheese` at all.\n",
    "\n",
    "*As an aside, I ran this experiment, and spaCy only had a 10% accuracy for classifying single-worded `FOOD` entities, failing with foods such as `hamburger` and `cheese`.*\n",
    "\n",
    "So let's filter the dataset further, such that **45%** are one-worded foods, **30%** are two-worded foods, and **25%** are three-worded foods."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00015-bc181835-a4b3-4f04-a8ad-adfc93d68ad8",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00017-73b30579-d061-44a1-8ae0-70fb3f9e5970",
    "output_cleared": false,
    "source_hash": "bc367ae7",
    "execution_millis": 15,
    "execution_start": 1608746172593,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# total number of foods\n",
    "total_num_foods = round(one_worded_foods.size / 45 * 100)\n",
    "\n",
    "# shuffle the 2-worded and 3-worded foods since we'll be slicing them\n",
    "two_worded_foods = two_worded_foods.sample(frac=1)\n",
    "three_worded_foods = three_worded_foods.sample(frac=1)\n",
    "\n",
    "# append the foods together \n",
    "foods = one_worded_foods.append(two_worded_foods[:round(total_num_foods * 0.30)]).append(three_worded_foods[:round(total_num_foods * 0.25)])\n",
    "\n",
    "# print the resulting sizes\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}-worded food entities:\", foods[foods.str.split().apply(len) == i + 1].size)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": "1-worded food entities: 1311\n2-worded food entities: 874\n3-worded food entities: 728\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split train and test food data\n",
    "At this point, we want to create different placeholders that we can insert our food entities into. I'll come back to update this once I'm testing [the project](https://niahealth.co/) with real users."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00011-b8657c77-7de4-4097-853e-dd32d78094d1",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-6d68af93-05bd-4c87-8928-8e5d5602a4f1",
    "output_cleared": false,
    "source_hash": "3bd29141",
    "execution_millis": 2,
    "execution_start": 1608746172613,
    "deepnote_cell_type": "code"
   },
   "source": [
    "food_templates = [\n",
    "    \"I ate my {}\",\n",
    "    \"I'm eating a {}\",\n",
    "    \"I just ate a {}\",\n",
    "    \"I only ate the {}\",\n",
    "    \"I'm done eating a {}\",\n",
    "    \"I've already eaten a {}\",\n",
    "    \"I just finished my {}\",\n",
    "    \"When I was having lunch I ate a {}\",\n",
    "    \"I had a {} and a {} today\",\n",
    "    \"I ate a {} and a {} for lunch\",\n",
    "    \"I made a {} and {} for lunch\",\n",
    "    \"I ate {} and {}\",\n",
    "    \"today I ate a {} and a {} for lunch\",\n",
    "    \"I had {} with my husband last night\",\n",
    "    \"I brought you some {} on my birthday\",\n",
    "    \"I made {} for yesterday's dinner\",\n",
    "    \"last night, a {} was sent to me with {}\",\n",
    "    \"I had {} yesterday and I'd like to eat it anyway\",\n",
    "    \"I ate a couple of {} last night\",\n",
    "    \"I had some {} at dinner last night\",\n",
    "    \"Last night, I ordered some {}\",\n",
    "    \"I made a {} last night\",\n",
    "    \"I had a bowl of {} with {} and I wanted to go to the mall today\",\n",
    "    \"I brought a basket of {} for breakfast this morning\",\n",
    "    \"I had a bowl of {}\",\n",
    "    \"I ate a {} with {} in the morning\",\n",
    "    \"I made a bowl of {} for my breakfast\",\n",
    "    \"There's {} for breakfast in the bowl this morning\",\n",
    "    \"This morning, I made a bowl of {}\",\n",
    "    \"I decided to have some {} as a little bonus\",\n",
    "    \"I decided to enjoy some {}\",\n",
    "    \"I've decided to have some {} for dessert\",\n",
    "    \"I had a {}, a {} and {} at home\",\n",
    "    \"I took a {}, {} and {} on the weekend\",\n",
    "    \"I ate a {} with {} and {} just now\",\n",
    "    \"Last night, I ate an {} with {} and {}\",\n",
    "    \"I tasted some {}, {} and {} at the office\",\n",
    "    \"There's a basket of {}, {} and {} that I consumed\",\n",
    "    \"I devoured a {}, {} and {}\",\n",
    "    \"I've already had a bag of {}, {} and {} from the fridge\"\n",
    "]"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll break up our food sentences (which contain our entities) into a training set and a test set. We also need the data to be in a specific format for training:\n",
    "```\n",
    "data = [\n",
    "    (\"I love chicken\", [(8, 13, \"FOOD\")]),\n",
    "    ... \n",
    "]\n",
    "```"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00013-3b312578-74c1-446a-ac85-2afd272824ea",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00011-7b984b03-b296-4c1a-85ec-b037e2eaa2a7",
    "output_cleared": false,
    "source_hash": "edd6433d",
    "execution_millis": 132,
    "execution_start": 1608746172715,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# create dictionaries to store the generated food combinations. Do note that one_food != one_worded_food. one_food == \"barbecue sauce\", one_worded_food == \"sauce\"\n",
    "TRAIN_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}\n",
    "\n",
    "TEST_FOOD_DATA = {\n",
    "    \"one_food\": [],\n",
    "    \"two_foods\": [],\n",
    "    \"three_foods\": []\n",
    "}\n",
    "\n",
    "# one_food, two_food, and three_food combinations will be limited to 167 sentences\n",
    "FOOD_SENTENCE_LIMIT = 167\n",
    "\n",
    "# helper function for deciding what dictionary and subsequent array to append the food sentence on to\n",
    "def get_food_data(count):\n",
    "    return {\n",
    "        1: TRAIN_FOOD_DATA[\"one_food\"] if len(TRAIN_FOOD_DATA[\"one_food\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"one_food\"],\n",
    "        2: TRAIN_FOOD_DATA[\"two_foods\"] if len(TRAIN_FOOD_DATA[\"two_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"two_foods\"],\n",
    "        3: TRAIN_FOOD_DATA[\"three_foods\"] if len(TRAIN_FOOD_DATA[\"three_foods\"]) < FOOD_SENTENCE_LIMIT else TEST_FOOD_DATA[\"three_foods\"],\n",
    "    }[count]\n",
    "\n",
    "# the pattern to replace from the template sentences\n",
    "pattern_to_replace = \"{}\"\n",
    "\n",
    "# shuffle the data before starting\n",
    "foods = foods.sample(frac=1)\n",
    "\n",
    "# the count that helps us decide when to break from the for loop\n",
    "food_entity_count = foods.size - 1\n",
    "\n",
    "# start the while loop, ensure we don't get an index out of bounds error\n",
    "while food_entity_count >= 2:\n",
    "    entities = []\n",
    "\n",
    "    # pick a random food template\n",
    "    sentence = food_templates[random.randint(0, len(food_templates) - 1)]\n",
    "\n",
    "    # find out how many braces \"{}\" need to be replaced in the template\n",
    "    matches = re.findall(pattern_to_replace, sentence)\n",
    "\n",
    "    # for each brace, replace with a food entity from the shuffled food data\n",
    "    for match in matches:\n",
    "        food = foods.iloc[food_entity_count]\n",
    "        food_entity_count -= 1\n",
    "\n",
    "        # replace the pattern, but then find the match of the food entity we just inserted\n",
    "        index = sentence.index(match)\n",
    "        match_span = (index, index + len(food))\n",
    "        sentence = sentence.replace(match, food, 1)\n",
    "        # match_span = re.search(food, sentence).span()\n",
    "\n",
    "        # use that match to find the index positions of the food entity in the sentence, append\n",
    "        entities.append((match_span[0], match_span[1], \"FOOD\"))\n",
    "\n",
    "    # append the sentence and the position of the entities to the correct dictionary and array\n",
    "    get_food_data(len(matches)).append((sentence, {\"entities\": entities}))"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00016-225f1580-c8a5-49f6-90dc-92df1354dff9",
    "output_cleared": false,
    "source_hash": "58d433aa",
    "execution_millis": 1,
    "execution_start": 1608746172896,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# print the number of food sentences, as well as an example sentence\n",
    "for key in TRAIN_FOOD_DATA:\n",
    "    print(\"{} {} sentences: {}\".format(len(TRAIN_FOOD_DATA[key]), key, TRAIN_FOOD_DATA[key][0]))"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "167 one_food sentences: ('I only ate the onion salt', {'entities': [(15, 25, 'FOOD')]})\n167 two_foods sentences: ('I ate pizzetta and cocoa crisps', {'entities': [(6, 14, 'FOOD'), (19, 31, 'FOOD')]})\n167 three_foods sentences: ('I devoured a punch, tabouli salad and chilositos', {'entities': [(13, 18, 'FOOD'), (20, 33, 'FOOD'), (38, 48, 'FOOD')]})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Nice, we now have ~500 training sentences, with each sentence either containing 1, 2, or 3 `FOOD` entities."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00017-d4068f9d-925d-4640-8c2b-1cd6c5dbf529",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00018-69a75a80-cc3e-46e2-a2f9-74707d42f2e1",
    "output_cleared": false,
    "source_hash": "249506d2",
    "execution_millis": 0,
    "execution_start": 1608746172898,
    "deepnote_cell_type": "code"
   },
   "source": [
    "for key in TEST_FOOD_DATA:\n",
    "    print(\"{} {} items: {}\".format(len(TEST_FOOD_DATA[key]), key, TEST_FOOD_DATA[key][0]))"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "930 one_food items: ('Last night, I ordered some quinee spread', {'entities': [(27, 40, 'FOOD')]})\n182 two_foods items: ('last night, a minis was sent to me with very veggie', {'entities': [(14, 19, 'FOOD'), (40, 51, 'FOOD')]})\n205 three_foods items: ('I had a fruits pies, a tamales beef and fancy cauliflower at home', {'entities': [(8, 19, 'FOOD'), (23, 35, 'FOOD'), (40, 57, 'FOOD')]})\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also have plenty of test data. Doesn't matter too much that it's not evenly distributed."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00019-482d6882-e813-4ba8-968b-86e9388bb796",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generating Revision Data\n",
    "\n",
    "As mentioned in the overview, we also need to generate sentences that contain spaCy entities. \n",
    "This helps us avoid the situation where the NER model is able to identify the `FOOD` entities, but [forgets](https://explosion.ai/blog/pseudo-rehearsal-catastrophic-forgetting) how to classify entities like `ORG` or `PERSON`.\n",
    "\n",
    "While `ORG` or `PERSON` isn't important for nutrition-tracking, other entities like `QUANTITY` and `CARDINAL` will help us associate foods with their quantities later on:\n",
    "\n",
    "- I ate `two` `slices of toast`."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00020-59b73684-abc4-47c3-8e9f-9d300d626a77",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing the revision data\n",
    "\n",
    "We use parts of Andrew Thompson's *All the news* dataset as the revision data. Specifically, the `articles1.csv` file's key, ID and title columns, as the other columns does not interest us and make the file heavier which could in turn increase the treatment speed. The reduction process is done by calling the [`article_trimmer`](./article_trimmer.py) file, which produces the trimmed `articles1_trimmed.csv` file we will use."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00022-47e06e3b-03f1-4a42-9197-892569ac3114",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00012-ca993d2d-602d-4ef6-a3c0-b01be950f74c",
    "output_cleared": false,
    "source_hash": "a311b976",
    "execution_millis": 43,
    "execution_start": 1608746172898,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# read in the revision data\n",
    "npr_df = pd.read_csv(\"articles_trimmed.csv\")\n",
    "\n",
    "# print row and column information\n",
    "npr_df.head()"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 12,
     "data": {
      "application/vnd.deepnote.dataframe.v2+json": {
       "row_count": 5,
       "column_count": 3,
       "columns": [
        {
         "name": "Unnamed: 0",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": 0,
          "max": 4,
          "histogram": [
           {
            "bin_start": 0,
            "bin_end": 0.4,
            "count": 1
           },
           {
            "bin_start": 0.4,
            "bin_end": 0.8,
            "count": 0
           },
           {
            "bin_start": 0.8,
            "bin_end": 1.2000000000000002,
            "count": 1
           },
           {
            "bin_start": 1.2000000000000002,
            "bin_end": 1.6,
            "count": 0
           },
           {
            "bin_start": 1.6,
            "bin_end": 2,
            "count": 0
           },
           {
            "bin_start": 2,
            "bin_end": 2.4000000000000004,
            "count": 1
           },
           {
            "bin_start": 2.4000000000000004,
            "bin_end": 2.8000000000000003,
            "count": 0
           },
           {
            "bin_start": 2.8000000000000003,
            "bin_end": 3.2,
            "count": 1
           },
           {
            "bin_start": 3.2,
            "bin_end": 3.6,
            "count": 0
           },
           {
            "bin_start": 3.6,
            "bin_end": 4,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "id",
         "dtype": "int64",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "min": 17283,
          "max": 17287,
          "histogram": [
           {
            "bin_start": 17283,
            "bin_end": 17283.4,
            "count": 1
           },
           {
            "bin_start": 17283.4,
            "bin_end": 17283.8,
            "count": 0
           },
           {
            "bin_start": 17283.8,
            "bin_end": 17284.2,
            "count": 1
           },
           {
            "bin_start": 17284.2,
            "bin_end": 17284.6,
            "count": 0
           },
           {
            "bin_start": 17284.6,
            "bin_end": 17285,
            "count": 0
           },
           {
            "bin_start": 17285,
            "bin_end": 17285.4,
            "count": 1
           },
           {
            "bin_start": 17285.4,
            "bin_end": 17285.8,
            "count": 0
           },
           {
            "bin_start": 17285.8,
            "bin_end": 17286.2,
            "count": 1
           },
           {
            "bin_start": 17286.2,
            "bin_end": 17286.6,
            "count": 0
           },
           {
            "bin_start": 17286.6,
            "bin_end": 17287,
            "count": 1
           }
          ]
         }
        },
        {
         "name": "title",
         "dtype": "object",
         "stats": {
          "unique_count": 5,
          "nan_count": 0,
          "categories": [
           {
            "name": "House Republicans Fret About Winning Their Health Care Suit",
            "count": 1
           },
           {
            "name": "Rift Between Officers and Residents as Killings Persist in South Bronx",
            "count": 1
           },
           {
            "name": "3 others",
            "count": 3
           }
          ]
         }
        },
        {
         "name": "_deepnote_index_column",
         "dtype": "int64"
        }
       ],
       "rows_top": [
        {
         "Unnamed: 0": 0,
         "id": 17283,
         "title": "House Republicans Fret About Winning Their Health Care Suit",
         "_deepnote_index_column": 0
        },
        {
         "Unnamed: 0": 1,
         "id": 17284,
         "title": "Rift Between Officers and Residents as Killings Persist in South Bronx",
         "_deepnote_index_column": 1
        },
        {
         "Unnamed: 0": 2,
         "id": 17285,
         "title": "Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial Bias, Dies at 106",
         "_deepnote_index_column": 2
        },
        {
         "Unnamed: 0": 3,
         "id": 17286,
         "title": "Among Deaths in 2016, a Heavy Toll in Pop Music",
         "_deepnote_index_column": 3
        },
        {
         "Unnamed: 0": 4,
         "id": 17287,
         "title": "Kim Jong-un Says North Korea Is Preparing to Test Long-Range Missile",
         "_deepnote_index_column": 4
        }
       ],
       "rows_bottom": null
      },
      "text/plain": "   Unnamed: 0     id                                              title\n0           0  17283  House Republicans Fret About Winning Their Hea...\n1           1  17284  Rift Between Officers and Residents as Killing...\n2           2  17285  Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...\n3           3  17286    Among Deaths in 2016, a Heavy Toll in Pop Music\n4           4  17287  Kim Jong-un Says North Korea Is Preparing to T...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>17283</td>\n      <td>House Republicans Fret About Winning Their Hea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>17284</td>\n      <td>Rift Between Officers and Residents as Killing...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>17285</td>\n      <td>Tyrus Wong, ‘Bambi’ Artist Thwarted by Racial ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>17286</td>\n      <td>Among Deaths in 2016, a Heavy Toll in Pop Music</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>17287</td>\n      <td>Kim Jong-un Says North Korea Is Preparing to T...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll keep sentences of a similar length to our generated food sentences."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00024-a2d5644f-479d-4924-aa7a-519e2f9012fa",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00025-84f715ff-7cd2-41ed-a0fa-ee2650b5be10",
    "output_cleared": false,
    "source_hash": "4c96f29f",
    "execution_millis": 4174,
    "execution_start": 1608746172943,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# create an nlp object as we'll use this to seperate the sentences and identify existing entities\n",
    "nlp = en_core_web_lg.load()"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00026-811aec36-7a66-4085-a71e-f9b371579a67",
    "output_cleared": false,
    "source_hash": "79ff89a",
    "execution_millis": 4572,
    "execution_start": 1608746177131,
    "deepnote_cell_type": "code"
   },
   "source": [
    "revision_texts = []\n",
    "\n",
    "# convert the articles to spacy objects to better identify the sentences. Disabled unneeded components. # takes ~ 4 minutes\n",
    "for doc in nlp.pipe(npr_df[\"title\"][:6000], batch_size=30, disable=[\"tagger\", \"ner\"]):\n",
    "    for sentence in doc.sents:\n",
    "        if  40 < len(sentence.text) < 80:\n",
    "            # some of the sentences had excessive whitespace in between words, so we're trimming that\n",
    "            revision_texts.append(\" \".join(re.split(\"\\s+\", sentence.text, flags=re.UNICODE)))"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "This takes a while. Unfortunately we need a lot of sentences because the entities we'll be able to identify aren't evenly distributed as we'll see later."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00027-15bb68c2-e793-497b-aacd-fb2ec7759c41",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00026-cefaabe7-dab7-44d6-8b3e-a2b0de961c4a",
    "output_cleared": false,
    "source_hash": "593d0e64",
    "execution_millis": 2442,
    "execution_start": 1608746181709,
    "deepnote_cell_type": "code"
   },
   "source": [
    "revisions = []\n",
    "\n",
    "# Use the existing spaCy model to predict the entities, then append them to revision\n",
    "for doc in nlp.pipe(revision_texts, batch_size=50, disable=[\"tagger\", \"parser\"]):\n",
    "    \n",
    "    # don't append sentences that have no entities\n",
    "    if len(doc.ents) > 0:\n",
    "        revisions.append((doc.text, {\"entities\": [(e.start_char, e.end_char, e.label_) for e in doc.ents]}))"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split train and test revision data\n",
    "In the previous step we filtered out sentences that were too short and too long. We've also used spaCy to predict the entities from the filtered sentences."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00027-1fd70631-4931-41bd-8fb6-cdc4dba459cb",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00028-7855d0aa-317a-491e-bdf7-65511d293d07",
    "output_cleared": false,
    "source_hash": "3efe7b39",
    "execution_millis": 2,
    "execution_start": 1608746184154,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# print an example of the revision sentence\n",
    "print(revisions[0][0])\n",
    "\n",
    "# print an example of the revision data\n",
    "print(revisions[0][1])"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": "House Republicans Fret About Winning Their Health Care Suit\n{'entities': [(0, 5, 'ORG'), (6, 17, 'NORP'), (29, 59, 'WORK_OF_ART')]}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When splitting the train and test data, we'll ensure that the revision training data has at least 100 examples of the different entity types."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00029-1b7b7fa9-a7c5-4958-9e1e-8b3edb506280",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00022-8dbd742a-da86-4e88-a534-9c561d2936a0",
    "output_cleared": false,
    "source_hash": "e324e89e",
    "execution_millis": 1,
    "execution_start": 1608746184171,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# create arrays to store the revision data\n",
    "TRAIN_REVISION_DATA = []\n",
    "TEST_REVISION_DATA = []\n",
    "\n",
    "# create dictionaries to keep count of the different entities\n",
    "TRAIN_ENTITY_COUNTER = {}\n",
    "TEST_ENTITY_COUNTER = {}\n",
    "\n",
    "# This will help distribute the entities (i.e. we don't want 1000 PERSON entities, but only 80 ORG entities)\n",
    "REVISION_SENTENCE_SOFT_LIMIT = 100\n",
    "\n",
    "# helper function for incrementing the revision counters\n",
    "def increment_revision_counters(entity_counter, entities):\n",
    "    for entity in entities:\n",
    "        label = entity[2]\n",
    "        if label in entity_counter:\n",
    "            entity_counter[label] += 1\n",
    "        else:\n",
    "            entity_counter[label] = 1\n",
    "\n",
    "random.shuffle(revisions)\n",
    "for revision in revisions:\n",
    "    # get the entities from the revision sentence\n",
    "    entities = revision[1][\"entities\"]\n",
    "\n",
    "    # simple hack to make sure spaCy entities don't get too one-sided\n",
    "    should_append_to_train_counter = 0\n",
    "    for _, _, label in entities:\n",
    "        if label in TRAIN_ENTITY_COUNTER and TRAIN_ENTITY_COUNTER[label] > REVISION_SENTENCE_SOFT_LIMIT:\n",
    "            should_append_to_train_counter -= 1\n",
    "        else:\n",
    "            should_append_to_train_counter += 1\n",
    "\n",
    "    # simple switch for deciding whether to append to train data or test data\n",
    "    if should_append_to_train_counter >= 0:\n",
    "        TRAIN_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TRAIN_ENTITY_COUNTER, entities)\n",
    "    else:\n",
    "        TEST_REVISION_DATA.append(revision)\n",
    "        increment_revision_counters(TEST_ENTITY_COUNTER, entities)"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are the entities and their counts that were captured in our revision training sentences:"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00033-4949170d-fca4-4715-9d44-bef3c2472282",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00023-27590c07-1a10-44a2-871c-3673b1bf417d",
    "output_cleared": false,
    "source_hash": "6462f521",
    "execution_millis": 4,
    "execution_start": 1608746184176,
    "deepnote_cell_type": "code"
   },
   "source": [
    "TRAIN_ENTITY_COUNTER"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 18,
     "data": {
      "text/plain": "{'PERSON': 216,\n 'CARDINAL': 112,\n 'NORP': 112,\n 'DATE': 102,\n 'GPE': 174,\n 'ORG': 191,\n 'WORK_OF_ART': 108,\n 'MONEY': 49,\n 'LOC': 66,\n 'TIME': 12,\n 'EVENT': 50,\n 'PRODUCT': 7,\n 'PERCENT': 5,\n 'FAC': 49,\n 'ORDINAL': 13,\n 'LANGUAGE': 7,\n 'LAW': 12,\n 'QUANTITY': 4}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here are the entities and counts captured in our revision test sentences. This just shows that our initial sentences had a large number of examples for `PERSON` but very few for `LAW`."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00035-c0229033-b0b8-4742-b243-de2dd86e6080",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00032-a20660bd-aba7-45b4-943c-7fea9f1188cb",
    "output_cleared": false,
    "source_hash": "24f6d6ab",
    "execution_millis": 42,
    "execution_start": 1608746184183,
    "deepnote_cell_type": "code"
   },
   "source": [
    "TEST_ENTITY_COUNTER"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 19,
     "data": {
      "text/plain": "{'PERSON': 1769,\n 'ORDINAL': 7,\n 'GPE': 1327,\n 'ORG': 1580,\n 'NORP': 365,\n 'DATE': 264,\n 'EVENT': 17,\n 'WORK_OF_ART': 397,\n 'CARDINAL': 322,\n 'LOC': 13,\n 'QUANTITY': 2,\n 'TIME': 36,\n 'LANGUAGE': 3,\n 'MONEY': 30,\n 'LAW': 8,\n 'FAC': 16,\n 'PRODUCT': 5,\n 'PERCENT': 3}"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the NER Model"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00033-60f4868d-3c57-4d4d-a66f-25964bbd7172",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For every food sentence, I have revision sentences. I haven't actually seen guidance on what this should be, so this is one of those \"stir until good enough\" moments."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00034-09414e21-1649-4c92-88a1-9a46608a903a",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00035-7c4fa1f1-2640-41af-8149-825eb2e601b3",
    "output_cleared": false,
    "source_hash": "a109d7d7",
    "execution_millis": 1,
    "execution_start": 1608746184225,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# combine the food training data\n",
    "TRAIN_FOOD_DATA_COMBINED = TRAIN_FOOD_DATA[\"one_food\"] + TRAIN_FOOD_DATA[\"two_foods\"] + TRAIN_FOOD_DATA[\"three_foods\"]\n",
    "\n",
    "# print the length of the food training data\n",
    "print(\"FOOD\", len(TRAIN_FOOD_DATA_COMBINED))\n",
    "\n",
    "# print the length of the revision training data\n",
    "print(\"REVISION\", len(TRAIN_REVISION_DATA))\n",
    "\n",
    "# join and print the combined length\n",
    "TRAIN_DATA = TRAIN_REVISION_DATA + TRAIN_FOOD_DATA_COMBINED\n",
    "print(\"COMBINED\", len(TRAIN_DATA))"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "FOOD 501\nREVISION 690\nCOMBINED 1191\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00037-eecdfe5c-a2ed-4c4c-93f6-94b194f0c5cc",
    "output_cleared": false,
    "source_hash": "5e7ad532",
    "execution_millis": 311009,
    "execution_start": 1608746184226,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# add NER to the pipeline and the new label\n",
    "ner = nlp.get_pipe(\"ner\")\n",
    "ner.add_label(\"FOOD\")\n",
    "\n",
    "# Add a variable to check the saved model's proper loading.\n",
    "move_names = list(ner.move_names)"
   ],
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "Losses (1/30) {'ner': 8888.886799229342}\nLosses (2/30) {'ner': 8266.238695203734}\nLosses (3/30) {'ner': 7923.912851821631}\nLosses (4/30) {'ner': 7747.612954877317}\nLosses (5/30) {'ner': 7681.29616130027}\nLosses (6/30) {'ner': 7695.768008612096}\nLosses (7/30) {'ner': 7590.117270927411}\nLosses (8/30) {'ner': 7664.932885835879}\nLosses (9/30) {'ner': 7502.272864446044}\nLosses (10/30) {'ner': 7695.678195792949}\nLosses (11/30) {'ner': 7616.72611448355}\nLosses (12/30) {'ner': 7516.879115562886}\nLosses (13/30) {'ner': 7422.719203163142}\nLosses (14/30) {'ner': 7523.160950578749}\nLosses (15/30) {'ner': 7482.562100547366}\nLosses (16/30) {'ner': 7538.007331099361}\nLosses (17/30) {'ner': 7452.13256226154}\nLosses (18/30) {'ner': 7547.345305136405}\nLosses (19/30) {'ner': 7530.70740153268}\nLosses (20/30) {'ner': 7567.840608038008}\nLosses (21/30) {'ner': 7393.941079969984}\nLosses (22/30) {'ner': 7426.958505111281}\nLosses (23/30) {'ner': 7357.100718127331}\nLosses (24/30) {'ner': 7448.3114754774}\nLosses (25/30) {'ner': 7453.556312654633}\nLosses (26/30) {'ner': 7392.385564534561}\nLosses (27/30) {'ner': 7309.885923741385}\nLosses (28/30) {'ner': 7294.76974780811}\nLosses (29/30) {'ner': 7350.985057947109}\nLosses (30/30) {'ner': 7439.780279187544}\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# get the names of the components we want to disable during training\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "# start the training loop, only training NER\n",
    "epochs = 30\n",
    "optimizer = nlp.resume_training()\n",
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "    \n",
    "    # batch up the examples using spaCy's minibatc\n",
    "    for epoch in range(epochs):\n",
    "        examples = TRAIN_DATA\n",
    "        random.shuffle(examples)\n",
    "        batches = minibatch(examples, size=sizes)\n",
    "        losses = {}\n",
    "        \n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "\n",
    "        print(\"Losses ({}/{})\".format(epoch + 1, epochs), losses)"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00039-75cce44e-d124-4d19-8d3f-719ff1497230",
    "output_cleared": false,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "cell_id": "00036-6f95a6b4-ad7a-44ac-a84b-962e5d216763",
    "output_cleared": false,
    "source_hash": "692c313e",
    "execution_millis": 0,
    "execution_start": 1608746495236,
    "deepnote_cell_type": "code",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluating the Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "cell_id": "00041-6a5bbacc-dc01-4d49-9a04-908cb0a2c006",
    "output_cleared": false,
    "source_hash": "75c05687",
    "execution_millis": 40,
    "execution_start": 1608746495236,
    "deepnote_cell_type": "code",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We first try the recognition for some of *SpaCy*'s default named entities."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# display sentence involving original entities\n",
    "spacy.displacy.render(nlp(\"Apple is looking at buying U.K. startup for $1 billion\"), style=\"ent\")"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00044-1f5d61b9-6b95-47c7-8fb5-25a16d210057",
    "output_cleared": false,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then try the created `FOOD` named entity recognition, with the provided example and a sentence from our recipe generator."
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00042-81b97db4-fdaa-4730-8fc8-57fa5be98c20",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00042-4489d26f-dffe-4077-afde-f53b1a4286c9",
    "output_cleared": false,
    "source_hash": "269c11d",
    "execution_millis": 13451,
    "execution_start": 1608746495277,
    "deepnote_cell_type": "code"
   },
   "source": [
    "# display sentences involving target entity\n",
    "spacy.displacy.render(nlp(\"I had a hamburger and chips for lunch today.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I decided to have chocolate ice cream as a little treat for myself.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I ordered basmati rice, leaf spinach and cheese from Tesco yesterday\"), style=\"ent\")\n",
    "\n",
    "spacy.displacy.render(nlp(\"I brought you some MANCHEGO CHEESE\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp(\"I ate one teaspoon of olive oil\"), style=\"ent\")"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "cell_id": "00043-dacbdb2c-d032-4502-8fb9-a8027f51fbab",
    "output_cleared": false,
    "source_hash": "8ef59d90",
    "execution_millis": 1,
    "execution_start": 1608746508734,
    "deepnote_cell_type": "code",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Initial results seem pretty good, let's evaluate on a wider scale."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Food Entities"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00050-61f54b3b-bce8-4e18-9b3e-b8e1a537e55c",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# dictionary to hold our evaluation data\n",
    "food_evaluation = {\n",
    "    \"one_food\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0,\n",
    "    },\n",
    "    \"two_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"three_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "word_evaluation = {\n",
    "    \"1_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"2_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    },\n",
    "    \"3_worded_foods\": {\n",
    "        \"correct\": 0,\n",
    "        \"total\": 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# loop over data from our test food set (3 keys in total)\n",
    "for key in TEST_FOOD_DATA:\n",
    "    foods = TEST_FOOD_DATA[key]\n",
    "\n",
    "    for food in foods:\n",
    "        # extract the sentence and correct food entities according to our test data\n",
    "        sentence = food[0]\n",
    "        entities = food[1][\"entities\"]\n",
    "\n",
    "        # for each entity, use our updated model to make a prediction on the sentence\n",
    "        for entity in entities:\n",
    "            doc = nlp(sentence)\n",
    "            correct_text = sentence[entity[0]:entity[1]]\n",
    "            n_worded_food =  len(correct_text.split())\n",
    "\n",
    "            # if we find that there's a match for predicted entity and predicted text, increment correct counters\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                    food_evaluation[key][\"correct\"] += 1\n",
    "                    if n_worded_food > 0:\n",
    "                        word_evaluation[f\"{n_worded_food}_worded_foods\"][\"correct\"] += 1\n",
    "                    \n",
    "                    # this break is important, ensures that we're not double counting on a correct match\n",
    "                    break\n",
    "            \n",
    "            #  increment total counters after each entity loop\n",
    "            food_evaluation[key][\"total\"] += 1\n",
    "            if n_worded_food > 0:\n",
    "                word_evaluation[f\"{n_worded_food}_worded_foods\"][\"total\"] += 1"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00043-99cdcce0-47f4-4ab3-9d7f-8c2d100a7777",
    "output_cleared": false,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00044-e5e6f6ba-bcc4-44dc-80df-fc3d7e4f6177",
    "output_cleared": false,
    "source_hash": "a3b4e911",
    "execution_millis": 42502,
    "execution_start": 1608746508742,
    "deepnote_cell_type": "code"
   },
   "source": [
    "for key in word_evaluation:\n",
    "    correct = word_evaluation[key][\"correct\"]\n",
    "    total = word_evaluation[key][\"total\"]\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "food_total_sum = 0\n",
    "food_correct_sum = 0\n",
    "\n",
    "print(\"---\")\n",
    "for key in food_evaluation:\n",
    "    correct = food_evaluation[key][\"correct\"]\n",
    "    total = food_evaluation[key][\"total\"]\n",
    "    \n",
    "    food_total_sum += total\n",
    "    food_correct_sum += correct\n",
    "\n",
    "    print(f\"{key}: {correct / total * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal: {food_correct_sum/food_total_sum * 100:.2f}%\")"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "cell_id": "00046-e41f160a-e219-4447-8860-8a117e5d9375",
    "output_cleared": false,
    "source_hash": "5be4a06b",
    "execution_millis": 0,
    "execution_start": 1608746551289,
    "deepnote_cell_type": "code",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "These results are really positive. We're stumbling with `1_worded_foods` accuracy, though that's potentially because we had more testing data for `1_worded_foods`. \n",
    "Perhaps with more test examples for `2_worded_foods` and `three_worded_foods`, we'd also see that accuracy trend to ~91%."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluating Existing Entities"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00054-3d0c46c7-d3c7-4fde-a536-b8274fff49dc",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# dictionary which will be populated with the entities and result information\n",
    "entity_evaluation = {}\n",
    "\n",
    "# helper function to udpate the entity_evaluation dictionary\n",
    "def update_results(entity, metric):\n",
    "    if entity not in entity_evaluation:\n",
    "        entity_evaluation[entity] = {\"correct\": 0, \"total\": 0}\n",
    "    \n",
    "    entity_evaluation[entity][metric] += 1\n",
    "\n",
    "# same as before, see if entities from test set match what spaCy currently predicts\n",
    "for data in TEST_REVISION_DATA:\n",
    "    sentence = data[0]\n",
    "    entities = data[1][\"entities\"]\n",
    "\n",
    "    for entity in entities:\n",
    "        doc = nlp(sentence)\n",
    "        correct_text = sentence[entity[0]:entity[1]]\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == entity[2] and ent.text == correct_text:\n",
    "                update_results(ent.label_, \"correct\")\n",
    "                break\n",
    "\n",
    "        update_results(entity[2], \"total\")\n"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00057-f86c366b-1584-4b9e-adde-3440df36ba84",
    "output_cleared": false,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [],
    "cell_id": "00058-4c181f9a-7161-4fb4-9ab7-f784fb6b557a",
    "output_cleared": false,
    "source_hash": "93e95b6d",
    "execution_millis": 7337,
    "execution_start": 1608746812580,
    "deepnote_cell_type": "code"
   },
   "source": [
    "sum_total = 0\n",
    "sum_correct = 0\n",
    "\n",
    "for entity in entity_evaluation:\n",
    "    total = entity_evaluation[entity][\"total\"]\n",
    "    correct = entity_evaluation[entity][\"correct\"]\n",
    "\n",
    "    sum_total += total\n",
    "    sum_correct += correct\n",
    "    \n",
    "    print(\"{} | {:.2f}%\".format(entity, correct / total * 100))\n",
    "\n",
    "print()\n",
    "print(\"Overall accuracy: {:.2f}%\".format(sum_correct / sum_total * 100))"
   ],
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "These results are a little harder to interpret. After all, we're testing entities that the original spaCy model predicited for us. \n",
    "Those predicted entities may well be **wrong** since spaCy's accuracy is at around 86%. If 14% of the entities we're using to verify the accuracy of our new model are wrong, then where does that leave us?\n",
    "\n",
    "A better comparison would be to load in spaCy's original model and use that to predict against this test set and compare that accuracy  **%** to this one of **71%**. \n",
    "We could then use that as a benchmark for measuring how introducing `FOOD` entities deteriorates our model.\n",
    "\n",
    "*Note: The online notebook I used keeps crashing as I attempt to do this, so will do this on my local computer.*"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00056-9c6295c2-707f-4549-844b-eadc2efd7da0",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving the model"
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00046-af3b1dc7-e29b-4123-8400-f586b135d5dd",
    "output_cleared": false,
    "deepnote_cell_type": "markdown"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "output_dir = \"../models/\""
   ],
   "metadata": {
    "tags": [],
    "cell_id": "00044-f19577db-9721-48a7-bc1c-2da346cdbaec",
    "output_cleared": false,
    "deepnote_cell_type": "markdown",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nlp.meta[\"name\"] = \"food_entity_recognition\"\n",
    "nlp.to_disk(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Check the saved model\n",
    "\n",
    "Code snippet from the *SpaCy* official website: https://spacy.io/usage/training#section-saving-loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test the saved model\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the classes have loaded back consistently\n",
    "assert nlp2.get_pipe(\"ner\").move_names == move_names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display sentence involving original entities\n",
    "spacy.displacy.render(nlp2(\"Apple is looking at buying U.K. startup for $1 billion\"), style=\"ent\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# display sentences involving target entity\n",
    "spacy.displacy.render(nlp2(\"I had a hamburger and chips for lunch today.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp2(\"I decided to have chocolate ice cream as a little treat for myself.\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp2(\"I ordered basmati rice, leaf spinach and cheese from Tesco yesterday\"), style=\"ent\")\n",
    "\n",
    "spacy.displacy.render(nlp2(\"I brought you some MANCHEGO CHEESE\"), style=\"ent\")\n",
    "spacy.displacy.render(nlp2(\"I ate one teaspoon of olive oil\"), style=\"ent\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_sample = ['0.5 (8 ounce) package button mushrooms',\n",
    "                '0.5 (6 ounce) package pre-sliced portabella mushrooms', '1 teaspoon olive oil',\n",
    "                '1 teaspoon butter', '2 tablespoons finely chopped shallots',\n",
    "                '2 garlic cloves, chopped', '18 teaspoon salt', '14 wonton wrappers',\n",
    "                '1 teaspoon cornstarch', '12 cup 1% low-fat milk',\n",
    "                '1 tablespoon all-purpose flour',\n",
    "                '2 tablespoons grated fresh parmesan cheese', '1 tablespoon chopped fresh chives',\n",
    "                '18 teaspoon salt', '1 dash fresh ground black pepper']\n",
    "\n",
    "for sample in test_sample:\n",
    "    sample = re.sub(\"teaspoons?\", \"\\g<0> of\", sample)\n",
    "    sample = re.sub(\"tablespoons?\", \"\\g<0> of\", sample)\n",
    "    spacy.displacy.render(nlp2(f\"I ate {sample}\"), style=\"ent\")\n",
    "    \n",
    "    ingredients = [ent.text for ent in nlp2(f\"I ate {sample}\").ents]\n",
    "    print(ingredients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "orig_nbformat": 2,
  "deepnote_notebook_id": "2349ad22-a0ab-4193-ba7d-02d7ff2977d0",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  }
 }
}
